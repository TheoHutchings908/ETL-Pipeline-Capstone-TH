# ETL Pipeline Capstone Project ðŸ§€


## Overview

This repository contains a complete ETL (Extract, Transform, Load) pipeline and a Streamlit dashboard for visualizing car sales data. The pipeline extracts raw CSV data, transforms it, and loads it into a PostgreSQL database. The Streamlit app connects to the database to display interactive charts and metrics.

### User Stories

* **As a Data Engineer**, I want to run the ETL pipeline in Docker container.
* **As an Executive**, I want to download the filtered dataset as a CSV so that I can perform deeper ad-hoc analysis in my own tools.
* **As a QA Engineer**, I want to have automated tests for data quality (e.g., no missing dates, matching makes/models) so that regressions are caught early.

## Features

- **Dockerized** services for PostgreSQL, ETL, and Streamlit  
- ETL scripts to extract from CSV, transform data, and load into Postgres  
- Streamlit dashboard with filters, metrics, and visualizations  
- Configuration via environment variables and `.env` file  

## Prerequisites

- Docker Desktop (please be careful and do research on if your CPU is ok with running this application)  
- (Optional) Local Python environment for development  

## Project Structure

---
```
ETL-Pipeline-Capstone/
â”œâ”€â”€ Dockerfile.etl             # ETL service Dockerfile
â”œâ”€â”€ Dockerfile.streamlit       # Streamlit service Dockerfile
â”œâ”€â”€ docker-compose.yml         # Compose file orchestrating all services
â”œâ”€â”€ .env.example               # Example environment variables file
â”œâ”€â”€ setup.py                   # Package & entry-point setup
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ extract/           # CSV extraction logic
â”‚       â”œâ”€â”€ transform/         # Data cleaning & transformation
â”‚       â””â”€â”€ load/              # Postgres loading logic
â”‚   â””â”€â”€ utils/                 # Shared utilities (logging, config, etc.)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_etl.py             # ETL pipeline launcher
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ db_utils.py            # DB connection & queries
â”‚   â””â”€â”€ streamlit_app.py       # Streamlit dashboard
â”œâ”€â”€ data/                      # Raw CSV data input
â”œâ”€â”€ logs/                      # Log files generated at runtime
â”œâ”€â”€ tests/                     # Pytest test suite
â””â”€â”€ README.md                  # Project README (this file)
```

## Getting Started

1. **Clone the repo**

   ```bash
   git clone https://github.com/TheoHutchings908/ETL-Pipeline-Capstone.git
   cd ETL-Pipeline-Capstone
   ```

2. **Create `.env` file**  
   Copy `.env.example` or create a `.env` with:

   ```dotenv
   POSTGRES_USER=etl_user
   POSTGRES_PASSWORD=DfIsTheBest
   POSTGRES_DB=car_sales
   POSTGRES_HOST=db
   POSTGRES_PORT=5432
   DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
   ```

3. **Build & run services separately**  

   - **Start database & initialize schema**  
  
     ```bash
     docker-compose up -d db
     ```

     Wait until the `db` service is healthy.

   - **Run ETL to load data**  

     ```bash
     docker-compose run --rm etl
     ```

   - **Launch Streamlit dashboard**  

     ```bash
      docker-compose up streamlit
     ```

4. **Access the dashboard**  
   Open your browser and navigate to:

   ```
   http://localhost:8501
   ```

---

## what would i do differently?

* Spend more time sourcing a dataset thatâ€™s accessible via an API (so I can pull fresh data programmatically, rather than manually downloading a CSV).
* Deploy the Docker containers on a cloud to automate ETL runs and keep the Streamlit app live.
* Implement a CI/CD pipeline (GitHub Actions, GitLab CI, etc.) to automatically test, build, and deploy both the ETL code and the Streamlit app on every merge.

## Author

Theo Hutchings  
[theohutchings2002@gmail.com](mailto:theohutchings2002@gmail.com) 
